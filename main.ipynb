{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JEVVsxFsh2ci"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from models import Trained_Resnet34, LoRAResnet34\n",
        "from data_loader import get_dataloaders, get_cifar100_dataloaders\n",
        "from train import accuracy, train_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "WARNING: If you get CUDA errors, restart the kernel (Kernel -> Restart Kernel)\n",
            "================================================================================\n",
            "PyTorch: 2.8.0+cu128\n",
            "CUDA: True\n",
            "GPU: NVIDIA GeForce RTX 3070 Ti\n",
            "GPU Memory Allocated: 0.00 GB\n",
            "GPU Memory Reserved: 0.00 GB\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# WARNING: RESTART KERNEL BEFORE RUNNING\n",
        "# Check device and clear GPU memory\n",
        "import gc\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"WARNING: If you get CUDA errors, restart the kernel (Kernel -> Restart Kernel)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    # Clear any existing GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
        "    print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf5sOYxEhvz3"
      },
      "source": [
        "# Hyperparameters Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rqRa4qQFhpst"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameters:\n",
            "  Batch Size: 96\n",
            "  Learning Rate: 0.001\n",
            "  Number of Epochs: 5\n",
            "  Optimizer: Adam\n",
            "  LoRA r: 8\n",
            "  LoRA alpha: 16\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters Configuration\n",
        "# Change these values to experiment with different settings\n",
        "\n",
        "BATCH_SIZE = 96\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 20\n",
        "OPTIMIZER = \"Adam\"\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "\n",
        "print(\"Hyperparameters:\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  Number of Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Optimizer: {OPTIMIZER}\")\n",
        "print(f\"  LoRA r: {LORA_R}\")\n",
        "print(f\"  LoRA alpha: {LORA_ALPHA}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sY6u2preu0-D"
      },
      "outputs": [],
      "source": [
        "# CIFAR-100 Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CIFAR-100 DataLoaders created:\n",
            "  Training samples: 50000\n",
            "  Validation samples: 5000\n",
            "  Test samples: 5000\n",
            "\n",
            "Data loaders created successfully!\n",
            "Training batches: 520\n",
            "Validation batches: 27\n",
            "Test batches: 27\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR-100 with separate train, validation, and test sets\n",
        "train_loader_cifar, val_loader_cifar, test_loader_cifar = get_cifar100_dataloaders(batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"\\nData loaders created successfully!\")\n",
        "print(f\"Training batches: {len(train_loader_cifar)}\")\n",
        "print(f\"Validation batches: {len(val_loader_cifar)}\")\n",
        "print(f\"Test batches: {len(test_loader_cifar)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison: Baseline ResNet vs LoRA ResNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TRAINING BASELINE RESNET MODEL\n",
            "================================================================================\n",
            "Model created with frozen backbone\n",
            "Device: cuda\n",
            "batch_size=96, lr=0.001, epochs=5\n",
            "Trainable params: 51,300 (0.2%)\n",
            "\n",
            "============================================================\n",
            "Epoch 1/5\n",
            "============================================================\n",
            "Iter 500 | Train Loss: 2.5238 | Train Acc: 0.4550 | Val Acc: 0.5826 | Val Loss: 1.5880\n",
            "  ✓ Saved best model (val_acc: 0.5826)\n",
            "\n",
            "Epoch 1 Summary:\n",
            "  Average Training Loss: 2.5860\n",
            "  Epoch Time: 96.68s\n",
            "\n",
            "============================================================\n",
            "Epoch 2/5\n",
            "============================================================\n",
            "Iter 1000 | Train Loss: 1.6464 | Train Acc: 0.4671 | Val Acc: 0.6210 | Val Loss: 1.3978\n",
            "  ✓ Saved best model (val_acc: 0.6210)\n",
            "\n",
            "Epoch 2 Summary:\n",
            "  Average Training Loss: 2.0835\n",
            "  Epoch Time: 35.45s\n",
            "\n",
            "============================================================\n",
            "Epoch 3/5\n",
            "============================================================\n",
            "Iter 1500 | Train Loss: 1.8629 | Train Acc: 0.4990 | Val Acc: 0.6306 | Val Loss: 1.3044\n",
            "  ✓ Saved best model (val_acc: 0.6306)\n",
            "\n",
            "Epoch 3 Summary:\n",
            "  Average Training Loss: 2.0031\n",
            "  Epoch Time: 36.54s\n",
            "\n",
            "============================================================\n",
            "Epoch 4/5\n",
            "============================================================\n",
            "Iter 2000 | Train Loss: 2.0707 | Train Acc: 0.5169 | Val Acc: 0.6402 | Val Loss: 1.2670\n",
            "  ✓ Saved best model (val_acc: 0.6402)\n",
            "\n",
            "Epoch 4 Summary:\n",
            "  Average Training Loss: 1.9680\n",
            "  Epoch Time: 37.58s\n",
            "\n",
            "============================================================\n",
            "Epoch 5/5\n",
            "============================================================\n",
            "Iter 2500 | Train Loss: 1.9284 | Train Acc: 0.5119 | Val Acc: 0.6354 | Val Loss: 1.2332\n",
            "\n",
            "Epoch 5 Summary:\n",
            "  Average Training Loss: 1.9410\n",
            "  Epoch Time: 33.09s\n",
            "\n",
            "============================================================\n",
            "Training Complete!\n",
            "Best validation accuracy: 0.6402 (Epoch 4)\n",
            "Total training time: 3.99 minutes\n",
            "Average time per epoch: 47.87s\n",
            "Best model saved to: baseline_best.pth\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 211, in collate\n    return [\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 270, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\storage.py\", line 1203, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\storage.py\", line 414, in _new_shared\n    return cls._new_using_filename_cpu(size)\nRuntimeError: Couldn't open shared file mapping: <torch_26232_145590723_0>, error code: <1455>\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Test accuracy\u001b[39;00m\n\u001b[0;32m     27\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m baseline_test_acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader_cifar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m baseline_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m baseline_test_acc\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBaseline Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbaseline_test_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mm:\\FUCK SCHOOL\\Foundation ML\\train.py:14\u001b[0m, in \u001b[0;36maccuracy\u001b[1;34m(model, data_loader, device, max_batches)\u001b[0m\n\u001b[0;32m     12\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (imgs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m max_batches \u001b[38;5;129;01mand\u001b[39;00m batch_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_batches:\n\u001b[0;32m     16\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mm:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
            "File \u001b[1;32mm:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1516\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1514\u001b[0m worker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mm:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1551\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data, worker_idx)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1551\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[1;32mm:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\_utils.py:769\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 211, in collate\n    return [\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 270, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\storage.py\", line 1203, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(\n  File \"m:\\FUCK SCHOOL\\Foundation ML\\.venv\\lib\\site-packages\\torch\\storage.py\", line 414, in _new_shared\n    return cls._new_using_filename_cpu(size)\nRuntimeError: Couldn't open shared file mapping: <torch_26232_145590723_0>, error code: <1455>\n"
          ]
        }
      ],
      "source": [
        "# Train Baseline ResNet Model\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING BASELINE RESNET MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Clear GPU memory aggressively\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "gc.collect()\n",
        "\n",
        "baseline_model = Trained_Resnet34(num_classes=100)\n",
        "\n",
        "baseline_metrics = train_model(\n",
        "    baseline_model,\n",
        "    train_loader_cifar,\n",
        "    val_loader_cifar,\n",
        "    optimizer_name=OPTIMIZER,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    model_save_path=\"baseline_best.pth\",\n",
        "    plot=False\n",
        ")\n",
        "\n",
        "# Test accuracy\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "baseline_test_acc = accuracy(baseline_model, test_loader_cifar, device)\n",
        "baseline_metrics['test_acc'] = baseline_test_acc\n",
        "print(f\"\\nBaseline Test Accuracy: {baseline_test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train LoRA ResNet Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TRAINING LORA RESNET MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Clear GPU memory (keeping baseline model for comparison)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB allocated\")\n",
        "\n",
        "lora_model = LoRAResnet34(num_classes=100, lora_r=LORA_R, lora_alpha=LORA_ALPHA)\n",
        "\n",
        "lora_metrics = train_model(\n",
        "    lora_model,\n",
        "    train_loader_cifar,\n",
        "    val_loader_cifar,\n",
        "    optimizer_name=OPTIMIZER,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    model_save_path=\"lora_best.pth\",\n",
        "    plot=False\n",
        ")\n",
        "\n",
        "# Test accuracy\n",
        "lora_test_acc = accuracy(lora_model, test_loader_cifar, device)\n",
        "lora_metrics['test_acc'] = lora_test_acc\n",
        "print(f\"\\nLoRA Test Accuracy: {lora_test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# Training Accuracy\n",
        "axes[0, 0].plot(baseline_metrics['iters'], baseline_metrics['train_acc'], \n",
        "                label='Baseline ResNet', marker='o', markersize=4, linewidth=2)\n",
        "axes[0, 0].plot(lora_metrics['iters'], lora_metrics['train_acc'], \n",
        "                label='LoRA ResNet', marker='s', markersize=4, linewidth=2)\n",
        "axes[0, 0].set_title('Training Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Iterations')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Validation Accuracy\n",
        "axes[0, 1].plot(baseline_metrics['iters'], baseline_metrics['val_acc'], \n",
        "                label='Baseline ResNet', marker='o', markersize=4, linewidth=2)\n",
        "axes[0, 1].plot(lora_metrics['iters'], lora_metrics['val_acc'], \n",
        "                label='LoRA ResNet', marker='s', markersize=4, linewidth=2)\n",
        "axes[0, 1].set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Iterations')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Training Loss\n",
        "axes[1, 0].plot(baseline_metrics['iters'], baseline_metrics['train_loss'], \n",
        "                label='Baseline ResNet', marker='o', markersize=4, linewidth=2)\n",
        "axes[1, 0].plot(lora_metrics['iters'], lora_metrics['train_loss'], \n",
        "                label='LoRA ResNet', marker='s', markersize=4, linewidth=2)\n",
        "axes[1, 0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Iterations')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Trainable Parameters Comparison\n",
        "models = ['Baseline\\nResNet', 'LoRA\\nResNet']\n",
        "params = [baseline_metrics['trainable_params']/1000, lora_metrics['trainable_params']/1000]\n",
        "axes[1, 1].bar(models, params, alpha=0.8, color=['#1f77b4', '#ff7f0e'])\n",
        "axes[1, 1].set_title('Trainable Parameters', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Parameters (thousands)')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display comparison results\n",
        "print(\"=\"*80)\n",
        "print(\"BASELINE RESNET RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Trainable Params: {baseline_metrics['trainable_params']:,}\")\n",
        "print(f\"Best Val Accuracy: {baseline_metrics['best_val_acc']:.4f}\")\n",
        "print(f\"Test Accuracy: {baseline_metrics['test_acc']:.4f}\")\n",
        "print(f\"Total Training Time: {baseline_metrics['total_time']/60:.2f} min\")\n",
        "print(f\"Best Epoch: {baseline_metrics['best_epoch']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LORA RESNET RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Trainable Params: {lora_metrics['trainable_params']:,}\")\n",
        "print(f\"Best Val Accuracy: {lora_metrics['best_val_acc']:.4f}\")\n",
        "print(f\"Test Accuracy: {lora_metrics['test_acc']:.4f}\")\n",
        "print(f\"Total Training Time: {lora_metrics['total_time']/60:.2f} min\")\n",
        "print(f\"Best Epoch: {lora_metrics['best_epoch']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "param_reduction = 100 * (1 - lora_metrics['trainable_params'] / baseline_metrics['trainable_params'])\n",
        "acc_diff = lora_metrics['test_acc'] - baseline_metrics['test_acc']\n",
        "time_diff = lora_metrics['total_time'] - baseline_metrics['total_time']\n",
        "\n",
        "print(f\"Parameter Reduction: {param_reduction:.1f}%\")\n",
        "print(f\"Accuracy Difference: {acc_diff:+.4f} (LoRA - Baseline)\")\n",
        "print(f\"Time Difference: {time_diff:+.2f}s (LoRA - Baseline)\")\n",
        "print(f\"\\nLoRA achieves {lora_metrics['test_acc']/baseline_metrics['test_acc']*100:.1f}% of baseline accuracy\")\n",
        "print(f\"with only {lora_metrics['trainable_params']/baseline_metrics['trainable_params']*100:.1f}% of trainable parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.9.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
